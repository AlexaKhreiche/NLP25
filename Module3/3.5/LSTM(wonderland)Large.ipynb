{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3784fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163947\n",
      "Total Vocab:  65\n",
      "Total Patterns:  163847\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad5469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 2.8633\n",
      "Epoch 1: loss improved from inf to 2.86332, saving model to weights-improvement-01-2.8633-bigger.hdf5\n",
      "2561/2561 [==============================] - 5167s 2s/step - loss: 2.8633\n",
      "Epoch 2/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 2.5538 \n",
      "Epoch 2: loss improved from 2.86332 to 2.55384, saving model to weights-improvement-02-2.5538-bigger.hdf5\n",
      "2561/2561 [==============================] - 26062s 10s/step - loss: 2.5538\n",
      "Epoch 3/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 2.3774\n",
      "Epoch 3: loss improved from 2.55384 to 2.37739, saving model to weights-improvement-03-2.3774-bigger.hdf5\n",
      "2561/2561 [==============================] - 7082s 3s/step - loss: 2.3774\n",
      "Epoch 4/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 2.2364\n",
      "Epoch 4: loss improved from 2.37739 to 2.23639, saving model to weights-improvement-04-2.2364-bigger.hdf5\n",
      "2561/2561 [==============================] - 2407s 940ms/step - loss: 2.2364\n",
      "Epoch 5/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 2.1308\n",
      "Epoch 5: loss improved from 2.23639 to 2.13081, saving model to weights-improvement-05-2.1308-bigger.hdf5\n",
      "2561/2561 [==============================] - 2033s 794ms/step - loss: 2.1308\n",
      "Epoch 6/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 2.0484\n",
      "Epoch 6: loss improved from 2.13081 to 2.04836, saving model to weights-improvement-06-2.0484-bigger.hdf5\n",
      "2561/2561 [==============================] - 2060s 804ms/step - loss: 2.0484\n",
      "Epoch 7/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 1.9788\n",
      "Epoch 7: loss improved from 2.04836 to 1.97885, saving model to weights-improvement-07-1.9788-bigger.hdf5\n",
      "2561/2561 [==============================] - 6898s 3s/step - loss: 1.9788\n",
      "Epoch 8/50\n",
      "2561/2561 [==============================] - ETA: 0s - loss: 1.9246\n",
      "Epoch 8: loss improved from 1.97885 to 1.92458, saving model to weights-improvement-08-1.9246-bigger.hdf5\n",
      "2561/2561 [==============================] - 4897s 2s/step - loss: 1.9246\n",
      "Epoch 9/50\n",
      " 495/2561 [====>.........................] - ETA: 1:06:51 - loss: 1.8724"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-XX-X.XXXX-bigger.hdf5\" #Replace it with the less loss weights file\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df23fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
